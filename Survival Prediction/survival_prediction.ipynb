{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from scipy.stats import weibull_min,weibull_max\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Input, multiply,Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.utils import concordance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 5 5fold image features\n",
    "stage12_fold1=pd.read_pickle(path_to_fold1_feature_pickle)\n",
    "stage12_fold2=pd.read_pickle(path_to_fold2_feature_pickle)\n",
    "stage12_fold3=pd.read_pickle(path_to_fold3_feature_pickle)\n",
    "stage12_fold4=pd.read_pickle(path_to_fold4_feature_pickle)\n",
    "stage12_fold5=pd.read_pickle(path_to_fold5_feature_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([stage12_fold1,stage12_fold2,stage12_fold3,stage12_fold4])\n",
    "test_df = stage12_fold5\n",
    "train_df=train_df.reset_index(drop=True)\n",
    "test_df=test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mil_Attention(Layer):\n",
    "    \"\"\"\n",
    "    # copy from https://github.com/utayao/Atten_Deep_MIL\n",
    "    Mil Attention Mechanism\n",
    "    This layer contains Mil Attention Mechanism\n",
    "    # Input Shape\n",
    "        2D tensor with shape: (batch_size, input_dim)\n",
    "    # Output Shape\n",
    "        2D tensor with shape: (1, units)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, L_dim, output_dim, kernel_initializer='glorot_uniform', kernel_regularizer=None,\n",
    "                    use_bias=True, use_gated=False, **kwargs):\n",
    "        self.L_dim = L_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.use_gated = use_gated\n",
    "\n",
    "        self.v_init = initializers.get(kernel_initializer)\n",
    "        self.w_init = initializers.get(kernel_initializer)\n",
    "        self.u_init = initializers.get(kernel_initializer)\n",
    "\n",
    "\n",
    "        self.v_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.w_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.u_regularizer = regularizers.get(kernel_regularizer)\n",
    "\n",
    "        super(Mil_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "\n",
    "        self.V = self.add_weight(shape=(input_dim, self.L_dim),\n",
    "                                      initializer=self.v_init,\n",
    "                                      name='v',\n",
    "                                      regularizer=self.v_regularizer,\n",
    "                                      trainable=True)\n",
    "\n",
    "\n",
    "        self.w = self.add_weight(shape=(self.L_dim, 1),\n",
    "                                    initializer=self.w_init,\n",
    "                                    name='w',\n",
    "                                    regularizer=self.w_regularizer,\n",
    "                                    trainable=True)\n",
    "\n",
    "\n",
    "        if self.use_gated:\n",
    "            self.U = self.add_weight(shape=(input_dim, self.L_dim),\n",
    "                                     initializer=self.u_init,\n",
    "                                     name='U',\n",
    "                                     regularizer=self.u_regularizer,\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            self.U = None\n",
    "\n",
    "        self.input_built = True\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        n, d = x.shape\n",
    "        ori_x = x\n",
    "        # do Vhk^T\n",
    "        x = k.tanh(k.dot(x, self.V)) # (2,64)\n",
    "\n",
    "        if self.use_gated: #no gate\n",
    "            gate_x = k.sigmoid(k.dot(ori_x, self.U))\n",
    "            ac_x = x * gate_x\n",
    "        else:\n",
    "            ac_x = x\n",
    "\n",
    "        # do w^T x\n",
    "        soft_x = k.dot(ac_x, self.w)  # (2,64) * (64, 1) = (2,1)\n",
    "        alpha = k.softmax(k.transpose(soft_x)) # (2,1)  #change\n",
    "        alpha = k.transpose(alpha)\n",
    "        return alpha\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 2\n",
    "        shape[1] = self.output_dim\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'v_initializer': initializers.serialize(self.V.initializer),\n",
    "            'w_initializer': initializers.serialize(self.w.initializer),\n",
    "            'v_regularizer': regularizers.serialize(self.v_regularizer),\n",
    "            'w_regularizer': regularizers.serialize(self.w_regularizer),\n",
    "            'use_bias': self.use_bias\n",
    "        }\n",
    "        base_config = super(Mil_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Last_Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Attention Activation\n",
    "    This layer contains a FC layer which only has one neural with sigmoid actiavtion\n",
    "    and MIL pooling. The input of this layer is instance features. Then we obtain\n",
    "    instance scores via this FC layer. And use MIL pooling to aggregate instance scores\n",
    "    into bag score that is the output of Score pooling layer.\n",
    "    This layer is used in mi-Net.\n",
    "    # Arguments\n",
    "        output_dim: Positive integer, dimensionality of the output space\n",
    "        kernel_initializer: Initializer of the `kernel` weights matrix\n",
    "        bias_initializer: Initializer of the `bias` weights\n",
    "        kernel_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "        bias_regularizer: Regularizer function applied to the `bias` weights\n",
    "        use_bias: Boolean, whether use bias or not\n",
    "        pooling_mode: A string,\n",
    "                      the mode of MIL pooling method, like 'max' (max pooling),\n",
    "                      'ave' (average pooling), 'lse' (log-sum-exp pooling)\n",
    "    # Input shape\n",
    "        2D tensor with shape: (batch_size, input_dim)\n",
    "    # Output shape\n",
    "        2D tensor with shape: (1, units)\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                    kernel_regularizer=None, bias_regularizer=None,\n",
    "                    use_bias=True, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        super(Last_Sigmoid, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.output_dim),\n",
    "                                        initializer=self.kernel_initializer,\n",
    "                                        name='kernel',\n",
    "                                        regularizer=self.kernel_regularizer)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.input_built = True\n",
    "    \n",
    "    def activate(ab):\n",
    "        a = k.exp(ab[:, 0])\n",
    "        b = k.softplus(ab[:, 1])\n",
    "        a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "        b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "        return k.concatenate((a, b), axis=1)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        n, d = x.shape\n",
    "        x = k.sum(x, axis=0, keepdims=True)\n",
    "        # compute instance-level score\n",
    "        x = k.dot(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x = k.bias_add(x, self.bias)\n",
    "\n",
    "        # sigmoid\n",
    "        out = activate(x) #change\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 2\n",
    "        shape[1] = self.output_dim\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel.initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'use_bias': self.use_bias\n",
    "        }\n",
    "        base_config = super(Last_Sigmoid, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, patient_bar, labels,status, batch_size=32, dim=(299,299), n_channels=3,\n",
    "                 folder='train',shuffle=False):\n",
    "        \n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.status = status\n",
    "        self.patient_bar=patient_bar\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.folder = folder\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.patient_bar) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        patient_bar_temp = [self.patient_bar[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X_person, y  = self.__data_generation(indexes)\n",
    "        \n",
    "       \n",
    "        return X_person, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.patient_bar))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, patient_bar_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "\n",
    "        train_df = pd.read_pickle('train_df.pkl')\n",
    "        test_df = pd.read_pickle('test_df.pkl')\n",
    "                \n",
    "        \n",
    "        real_day=list(train_df['Days'])\n",
    "        int_day = map(int, real_day)\n",
    "        real_day=list(int_day)\n",
    "        day_mean=statistics.mean(real_day)\n",
    "        day_std=statistics.stdev(real_day)\n",
    "        Days_std=[((i -day_mean)/day_std)+n for i in real_day] #please input n (make sure Days_std >0)\n",
    "\n",
    "        train_df['Days_std'] = Days_std\n",
    "        train_df=train_df.reset_index(drop=True)\n",
    "\n",
    "        test_day=list(test_df['Days'])\n",
    "        int_test = list(map(int, test_day))\n",
    "        Days_std_test=[((i -day_mean)/day_std)+n for i in int_test] #please input n\n",
    "\n",
    "        test_df['Days_std'] = Days_std_test\n",
    "        test_df=test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        feature_columns=train_df.columns[xi:xj]  #select feature columns\n",
    "        \n",
    "        X_person = []\n",
    "        \n",
    "        y=[]\n",
    "        y_d = []\n",
    "        y_s = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(patient_bar_temp):\n",
    "            train_array=[]\n",
    "            \n",
    "            if self.folder == 'train_df':\n",
    "                per_person=train_df.loc[train_df['bcr_patient_barcode'] == self.patient_bar[ID]] #select a person\n",
    "            \n",
    "            else :\n",
    "                per_person=test_df.loc[test_df['bcr_patient_barcode'] == self.patient_bar[ID]] #select a person\n",
    "                \n",
    "            per_person_array=(per_person[feature_columns]).to_numpy() #per person features to array\n",
    "            train_array.append(per_person_array)\n",
    "            \n",
    "            X_person.append(train_array)\n",
    "            \n",
    "\n",
    "            \n",
    "            y_d.append(list(per_person['Days'])[0])\n",
    "            y_s.append(list(per_person['vital_status'])[0])\n",
    "\n",
    "            \n",
    "            \n",
    "        X_person = np.array(X_person).squeeze()\n",
    "        \n",
    "              \n",
    "        y_d = np.array(y_d).astype('float')[:, np.newaxis]\n",
    "        \n",
    "        y_s = np.array(y_s).astype('int')[:, np.newaxis]\n",
    "        y_temp = np.hstack((y_d, y_s))   \n",
    "        \n",
    "        y = tf.convert_to_tensor(y_temp, dtype=tf.float32)\n",
    "        \n",
    "        return X_person, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n:lambda  a:kappa\n",
    "def weib(x,a,n):\n",
    "    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "\n",
    "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
    "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
    "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)\n",
    "\n",
    "\n",
    "def weibull_loglik_continuous(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]  # death / live\n",
    "    ya = (y_ + 1e-35) / a_\n",
    "    return -1 * k.mean(u_ * (k.log(b_) + b_ * k.log(ya)) - k.pow(ya, b_))\n",
    "\n",
    "\n",
    "def activate(ab):\n",
    "    a = k.exp(ab[:, 0])\n",
    "    b = k.softplus(ab[:, 1])\n",
    "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "    return k.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['Days_std']\n",
    "train_status = train_df['vital_status']\n",
    "train_bar= list(set(train_df['bcr_patient_barcode']))\n",
    "\n",
    "test_labels = test_df['Days_std']\n",
    "test_status = test_df['vital_status']\n",
    "test_bar= list(set(test_df['bcr_patient_barcode']))\n",
    "\n",
    "params_train = {'dim': (299,299),'batch_size': 1,'n_channels': 3,'shuffle': False,'folder':'train_df'}\n",
    "params_test = {'dim': (299,299),'batch_size': 1,'n_channels': 3,'shuffle': False,'folder':'test_df'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(train_bar, train_labels,train_status, **params_train )\n",
    "validation_generator = DataGenerator(test_bar, test_labels,test_status, **params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(shape=(2048), dtype='float32', name='input')\n",
    "alpha = Mil_Attention(L_dim=2048, output_dim=2, kernel_regularizer=l2(0.000001), name='alpha', use_gated='False')(data_input)\n",
    "x_mul = multiply([alpha, data_input])\n",
    "out = Last_Sigmoid(output_dim=2, name='FC1_sigmoid')(x_mul)\n",
    "x_out = out\n",
    "model = Model(inputs=data_input, outputs=x_out)\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_function,  optimizer=optimizer) #please input loss function,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_generator,\n",
    "          validation_data=validation_generator,\n",
    "          epochs=epochs,  #please input epochs\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model.predict(training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'bcr_patient_barcode':  train_bar}\n",
    "infer_train_df = pd.DataFrame (data, columns = ['bcr_patient_barcode'])\n",
    "\n",
    "train_origin=train_df[['bcr_patient_barcode','Days','Days_std','vital_status']].drop_duplicates(subset=['bcr_patient_barcode'], keep='last')\n",
    "infer_train_df = pd.merge(infer_train_df, train_origin, on=['bcr_patient_barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_predict = np.resize(probabilities, (len(infer_train_df) , 2))\n",
    "\n",
    "\n",
    "\n",
    "train_result = np.concatenate((np.array(infer_train_df['Days'])[:, np.newaxis], train_predict), axis=1)\n",
    "\n",
    "predict_list=[]\n",
    "kappa_list=[]\n",
    "lambda_list=[]\n",
    "for i in range(0,len(train_result)):\n",
    "#pdf peak\n",
    "#     x = np.arange(0,15,0.0001)\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     per=x[np.argmax(per_list)]\n",
    "    \n",
    "#cdf 0.5\n",
    "    per=weibull_min.ppf(0.5, scale=train_result[i][1], c=train_result[i][2])\n",
    "\n",
    "#pdf monte carlo 10000 - y\n",
    "#     x = [random.uniform(0, 15) for i in range(20000)]\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     #new_list = np.append(per_list, per)\n",
    "#     per=statistics.mean(per_list)\n",
    "\n",
    "#pdf monte carlo 10000 - x\n",
    "#     x = np.arange(0,15,0.0001)\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     per_list=per_list/sum(per_list)\n",
    "#     x_list = np.random.choice(x,size=10000, p = per_list.ravel())\n",
    "# #     per=statistics.mean(per_list)\n",
    "#     per=stats.mode(x_list)[0][0]\n",
    "    \n",
    "    \n",
    "    kappa=train_result[i][2]\n",
    "    lamda=train_result[i][1]\n",
    "    kappa_list.append(kappa)\n",
    "    lambda_list.append(lamda)\n",
    "    predict_list.append(per)\n",
    "\n",
    "infer_train_df['predict_std_day']=predict_list\n",
    "infer_train_df['lambda']=lambda_list\n",
    "infer_train_df['kappa']=kappa_list\n",
    "\n",
    "\n",
    "#C-index\n",
    "print(concordance_index(infer_train_df['Days'], infer_train_df['predict_std_day'], infer_train_df['vital_status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test = model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = {'bcr_patient_barcode':  test_bar}\n",
    "infer_test_df = pd.DataFrame (data_test, columns = ['bcr_patient_barcode'])\n",
    "\n",
    "test_origin=test_df[['bcr_patient_barcode','Days','Days_std','vital_status']].drop_duplicates(subset=['bcr_patient_barcode'], keep='last')\n",
    "infer_test_df = pd.merge(infer_test_df, test_origin, on=['bcr_patient_barcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = np.resize(probabilities_test, (len(infer_test_df) , 2))\n",
    "test_result = np.concatenate((np.array(infer_test_df['Days'])[:, np.newaxis], test_predict), axis=1)\n",
    "\n",
    "predict_list=[]\n",
    "kappa_list=[]\n",
    "lambda_list=[]\n",
    "for i in range(0,len(test_result)):\n",
    "#pdf peak\n",
    "#     x = np.arange(0,15,0.0001)\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     per=x[np.argmax(per_list)]\n",
    "    \n",
    "#cdf 0.5\n",
    "    per=weibull_min.ppf(0.5, scale=test_result[i][1], c=test_result[i][2])\n",
    "\n",
    "#pdf monte carlo 10000 - y\n",
    "#     x = [random.uniform(0, 15) for i in range(20000)]\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     #new_list = np.append(per_list, per)\n",
    "#     per=statistics.mean(per_list)\n",
    "\n",
    "#pdf monte carlo 10000 - x\n",
    "#     x = np.arange(0,15,0.0001)\n",
    "#     per_list=weibull_min.pdf(x, scale=train_result[i][1], c=train_result[i][2])\n",
    "#     per_list=per_list/sum(per_list)\n",
    "#     x_list = np.random.choice(x,size=10000, p = per_list.ravel())\n",
    "# #     per=statistics.mean(per_list)\n",
    "#     per=stats.mode(x_list)[0][0]\n",
    "    \n",
    "    \n",
    "    kappa=test_result[i][2]\n",
    "    lamda=test_result[i][1]\n",
    "    kappa_list.append(kappa)\n",
    "    lambda_list.append(lamda)\n",
    "    predict_list.append(per)\n",
    "\n",
    "infer_test_df['predict_std_day']=predict_list\n",
    "infer_test_df['lambda']=lambda_list\n",
    "infer_test_df['kappa']=kappa_list\n",
    "\n",
    "#C-index\n",
    "print(concordance_index(infer_test_df['Days'], infer_test_df['predict_std_day'], infer_test_df['vital_status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistics and kmcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_train_df['Days'] = [int(i) for i in infer_train_df['Days']]\n",
    "med_day=np.median(list(infer_train_df['predict_std_day']))\n",
    "med_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_test_df['Days'] = [int(i) for i in infer_test_df['Days']]\n",
    "# med_day=np.median(list(infer_test_df['predict_std_day']))\n",
    "# med_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_test_df[\"group\"]=''\n",
    "\n",
    "for i in range(0,len(infer_test_df)):\n",
    "    if infer_test_df['predict_std_day'][i] > med_day :\n",
    "        infer_test_df[\"group\"][i]=1 #big\n",
    "    else :\n",
    "        infer_test_df[\"group\"][i]=0 #small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=infer_test_df\n",
    "\n",
    "T_big=[]\n",
    "T_small=[]\n",
    "E_big=[]\n",
    "E_small=[]\n",
    "for i in range(0,len(result)):\n",
    "    if result['group'][i]==1: #big\n",
    "        T_big.append(result['Overall Survival (Months)'][i])\n",
    "        E_big.append(result['Overall Survival Status'][i])\n",
    "    else: #small\n",
    "        T_small.append(result['Overall Survival (Months)'][i])\n",
    "        E_small.append(result['Overall Survival Status'][i])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logrank_results = logrank_test(T_big, T_small, event_observed_A=E_big, event_observed_B=E_small)\n",
    "logrank_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "kmf.fit(T_small, event_observed=E_small, label=\"Shorter-term survivors\")\n",
    "kmf.plot(show_censors=True,ax=ax)\n",
    "\n",
    "kmf.fit(T_big, event_observed=E_big, label=\"Longer-term survivors\")\n",
    "kmf.plot(show_censors=True,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
